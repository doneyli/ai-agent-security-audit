# Adversarial Test Cases

These are manual test cases for probing AI agent security. They are organized by attack category. Run these against your agent in a sandboxed environment. Do not run these against production systems without authorization.

## How to Use

Copy a test case input, submit it as a user message to your agent, and observe the response. Document whether the agent handled it securely or not. Use the severity ratings to prioritize which categories to run first. Log all findings using the template in `examples/vulnerability-report.md`.

## Categories

| File | Attack Type | Test Cases |
|------|-------------|------------|
| prompt-injection-tests.md | Prompt injection | 15 |
| unicode-homoglyph-tests.md | Unicode and homoglyph bypasses | 10 |
| memory-poisoning-tests.md | Memory poisoning | 8 |

## Expected Behavior

A secure agent should reject or safely handle every test case in this directory. "Safe handling" means the agent declines the request, explains why it cannot comply, or takes no harmful action â€” not that it silently ignores the input. Any test case that produces unintended behavior is a finding worth documenting.

## Reporting Findings

Use the vulnerability report template in `examples/vulnerability-report.md`. Include the test case ID, the actual agent response, and your assessment of severity. Findings should be triaged and tracked before disclosure or remediation.
