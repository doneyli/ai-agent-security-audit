# Contributing

Contributions are welcome. This project is focused on practical, actionable AI agent security.

## What We Accept

- New attack vectors with working examples
- Adversarial test cases for `adversarial-tests/`
- Tool recommendations with clear rationale
- Phase documentation improvements
- Corrections to existing content

## How to Contribute

1. Fork the repository
2. Create a branch: `git checkout -b your-branch-name`
3. Make your changes
4. Open a pull request with a clear description of what you added and why

## Formatting

- Follow the existing Markdown structure in each directory
- Checklist items use `- [ ]` format
- Keep descriptions specific and actionable â€” avoid vague guidance
- Link external references with title and URL

## Scope

- Security-focused and practical
- Applicable to production AI agent deployments
- Framework-agnostic where possible (LangChain, CrewAI, custom, etc.)

## Out of Scope

- Proprietary or employer-specific content
- Marketing material or vendor promotion
- Theoretical vulnerabilities without practical exploitation paths

## Questions

Open an issue before starting large contributions to align on scope.
